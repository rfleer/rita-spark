{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d313ecb0-e355-4f58-8267-a5e473571ffa",
   "metadata": {},
   "source": [
    "# What is MapReduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ce2fa-2660-4634-b6ad-2d55dae30ac6",
   "metadata": {},
   "source": [
    " - http://www.techspot.co.in/2011/07/mapreduce-for-dummies.html\n",
    " - https://book.pythontips.com/en/latest/map_filter.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d7f40f-ce32-4334-aa6c-8eea0bfd09cc",
   "metadata": {},
   "source": [
    "# PySpark basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b86c6-5167-443b-9da8-0a1efe6079d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sc)\n",
    "\n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a443cab-69a6-43e4-be1b-a2ea70ef7c09",
   "metadata": {},
   "source": [
    "Resilient Distributed Dataset (RDD) is the basic abstraction in Spark. It is an immutable distributed collection of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3b8dc-a044-4acd-a923-e194ee623b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an RDD from a list of words\n",
    "RDD = sc.parallelize([\"Spark\", \"is\", \"a\", \"framework\", \"for\", \"Big Data processing\"])\n",
    "\n",
    "# Print out the type of the created object\n",
    "print(\"The type of RDD is\", type(RDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f84699-ebca-4452-8608-3cb976507c4d",
   "metadata": {},
   "source": [
    "PySpark can easily create RDDs from files that are stored in external storage devices such as HDFS (Hadoop Distributed File System), Amazon S3 buckets, etc. However, the most common method of creating RDD's is from files stored in your local file system. This method takes a file path and reads it as a collection of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d9c841-f553-4e36-96dd-f5534dcb7470",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/FileStore/tables/swearwords-1.txt'\n",
    "\n",
    "# Print the file_path\n",
    "print(\"The file_path is\", file_path)\n",
    "\n",
    "# Create a fileRDD from file_path\n",
    "fileRDD = sc.textFile(file_path)\n",
    "\n",
    "# Check the type of fileRDD\n",
    "print(\"The file type of fileRDD is\", type(fileRDD))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d69ce3-ecaa-4f87-9e53-9fdd5c509e55",
   "metadata": {},
   "source": [
    "SparkContext's textFile() method takes an optional second argument called minPartitions for specifying the minimum number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b824b3-33fb-4bc1-abf2-12318498035a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of partitions in fileRDD\n",
    "print(\"Number of partitions in fileRDD is\", fileRDD.getNumPartitions())\n",
    "\n",
    "# Create a fileRDD_part from file_path with 5 partitions\n",
    "fileRDD_part = sc.textFile(file_path, minPartitions=5)\n",
    "\n",
    "# Check the number of partitions in fileRDD_part\n",
    "print(\"Number of partitions in fileRDD_part is\", fileRDD_part.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133230de-0ab9-47b0-940d-335251fc5fe8",
   "metadata": {},
   "source": [
    "The main method by which you can manipulate data in PySpark is using map(). The map() transformation takes in a function and applies it to each element in the RDD. It can be used to do any number of things, from fetching the website associated with each URL in our collection to just squaring the numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b375e49f-f820-47c0-9ecc-ad1dae068b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create map() transformation to cube numbers\n",
    "cubedRDD = RDD.map(lambda x: x**3)\n",
    "\n",
    "# Collect the results\n",
    "numbers_all = cubedRDD.collect()\n",
    "\n",
    "# Print the numbers from numbers_all\n",
    "for numb in numbers_all:\n",
    "    print(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31e6e63-9a43-44f5-bbb6-5e0abaae9cc7",
   "metadata": {},
   "source": [
    "The RDD transformation filter() returns a new RDD containing only the elements that satisfy a particular function. It is useful for filtering large datasets based on a keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a860d858-ba9a-4f45-9063-b6af18162a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the fileRDD to select lines with Spark keyword\n",
    "fileRDD_filter = fileRDD.filter(lambda line: 'Spark' in line)\n",
    "\n",
    "# How many lines are there in fileRDD?\n",
    "print(\"The total number of lines with the keyword Spark is\", fileRDD_filter.count())\n",
    "\n",
    "# Print the first four lines of fileRDD\n",
    "for line in fileRDD_filter.take(4): \n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342f1976-cee1-49b5-b0db-a24e4dd664c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatMap() transformation returns multiple values for each element in the original RDD\n",
    "\n",
    "RDD = sc.parallelize([\"hello world\", \"how are you\"])\n",
    "RDD_flatmap = RDD.flatMap(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0965e-9432-4b0d-8838-5bae2e4b8c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# union() gives possibility to split RDDs e.g. on condition, and then union back\n",
    "\n",
    "badRDD = inputRDD.filter(lambda x: \"ass\" in x.split())\n",
    "goodRDD = inputRDD.filter(lambda x: \"d1ck\" in x.split())\n",
    "combinedRDD = badRDD.union(goodRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d01319-446e-4ea3-983c-255f1f47dafa",
   "metadata": {},
   "source": [
    "One of the most popular pair RDD transformations is reduceByKey() which operates on key, value (k,v) pairs and merges the values for each key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f47f878-f782-40ab-a1fa-2cd992a5fd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PairRDD Rdd with key value pairs\n",
    "Rdd = sc.parallelize([(1,2),(3,4),(3,6),(4,5)])\n",
    "\n",
    "# Apply reduceByKey() operation on Rdd\n",
    "Rdd_Reduced = Rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# Iterate over the result and print the output\n",
    "for num in Rdd_Reduced.collect(): \n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1f79b6-f4a4-446c-ab53-17352ce8d14f",
   "metadata": {},
   "source": [
    "Many times it is useful to sort the pair RDD based on the key (for example word count which you'll see later in the chapter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fe622-8cc0-44c4-99a9-e77f75e7338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the reduced RDD with the key by descending order\n",
    "Rdd_Reduced_Sort = Rdd_Reduced.sortByKey(ascending=False)\n",
    "\n",
    "# Iterate over the result and retrieve all the elements of the RDD\n",
    "for num in Rdd_Reduced_Sort.collect():\n",
    "    print(\"Key {} has {} Counts\".format(num[0], num[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7f483f-d908-46c6-8603-3d047f915d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "groupByKey() groups all the values with the same key in the pair RDD\n",
    "\"\"\"\n",
    "\n",
    "airports = [(\"US\", \"JFK\"),(\"UK\", \"LHR\"),(\"FR\", \"CDG\"),(\"US\", \"SFO\")]\n",
    "regularRDD = sc.parallelize(airports)\n",
    "pairRDD_group = regularRDD.groupByKey().collect()\n",
    "for cont, air in pairRDD_group:\n",
    "    print(cont, list(air))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77209aaf-9ba7-4999-bc69-25358c7e33c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join() transformation joins the two pair RDDs based on their key\n",
    "\n",
    "RDD1 = sc.parallelize([(\"Messi\", 34),(\"Ronaldo\", 32),(\"Neymar\", 24)])\n",
    "RDD2 = sc.parallelize([(\"Ronaldo\", 80),(\"Neymar\", 120),(\"Messi\", 100)])\n",
    "RDD1.join(RDD2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f144bd5-08c4-46fa-82c9-1ca5380d8c1b",
   "metadata": {},
   "source": [
    "For many datasets, it is important to count the number of keys in a key/value dataset. For example, counting the number of countries where the product was sold or to show the most popular baby names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003cba7d-cde4-4512-b974-c488c8726cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the unique keys\n",
    "total = Rdd.countByKey()\n",
    "\n",
    "# What is the type of total?\n",
    "print(\"The type of total is\", type(total))\n",
    "\n",
    "# Iterate over the total and print the output\n",
    "for k, v in total.items(): \n",
    "    print(\"key\", k, \"has\", v, \"counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c916cb8b-c564-4978-a121-3a393536d239",
   "metadata": {},
   "source": [
    "# Creating your own streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb920d8-9b9f-4ddd-8f4b-7ac122e8f030",
   "metadata": {},
   "source": [
    "https://realpython.com/python-sockets/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
